{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a574c0-8034-4552-a61a-80de53050891",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "  \n",
    "A projection is a mathematical operation that transforms a data point from a high-dimensional space to \n",
    "a lower-dimensional space. In PCA, the projection is used to transform the data into a new set of orthogonal \n",
    "variables called principal components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3ee28-0381-4f93-99f7-85b60c56a159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311c2b2-5e85-4e28-aaaf-ba304e6cce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "   The optimization problem in PCA aims to find the directions of the principal components that maximize\n",
    "the variance of the data. This is achieved by computing the eigenvectors of the covariance matrix of the \n",
    "data and selecting the top k eigenvectors as the principal components. The optimization problem is solved\n",
    "using eigenvalue decomposition or singular value decomposition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa037a-d970-4c7a-b28d-ef920c3a47f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd98df5-5282-4f05-b21d-4df287250a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:\n",
    "  \n",
    "Covariance matrices are used in PCA to compute the eigenvectors and eigenvalues that define the principal components.\n",
    "The covariance matrix represents the relationships between the variables in the data and is used to measure the amount\n",
    "of variance shared by the variables. PCA uses the covariance matrix to find the directions of maximum variance in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baff7e-0513-4cff-b8fe-ce2e5d8f869a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646fe84d-005c-4ffa-877d-2e177f2dcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    " The choice of the number of principal components impacts the performance of PCA in several ways.\n",
    "Selecting too few principal components may result in loss of important information, while selecting\n",
    "too many may lead to overfitting and increased computational complexity. The optimal number of principal\n",
    "components can be determined by examining the explained variance ratio and selecting the number of components\n",
    "that explain most of the variance in the data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334e98a-d4af-46c3-a337-0d3e1ea71b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ba8ad-c642-4f3f-903f-7d2d31c8daa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "  PCA can be used in feature selection by identifying the most important features in a dataset, which\n",
    "are then used to create new variables or features that capture the most variability in the original data.\n",
    "This is done by finding the principal components of the data, which are linear combinations of the original\n",
    "features that explain the most variance in the data. The benefits of using PCA for feature selection include\n",
    "reducing the dimensionality of the data, removing redundant or irrelevant features, and improving the performance\n",
    "of machine learning models by reducing overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fcd11-8e3e-4325-af9b-a4deda2708e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc90ce1-f97a-4457-9bb3-481b63d36a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    "   Some common applications of PCA in data science and machine learning include image and signal processing,\n",
    "data compression, feature extraction, and anomaly detection. PCA is also commonly used in exploratory data \n",
    "analysis to identify patterns and relationships in high-dimensional data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba5c4c-2ae9-44e4-85f0-7e8c0fe628f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c509b37e-ab49-40cd-aaf2-30b4b5e2caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "7:\n",
    "  In PCA, spread refers to the distribution of the data along each principal component, while variance \n",
    "refers to the amount of variability in the data along each principal component. Spread and variance are\n",
    "related in that the spread of the data along each principal component is determined by the variance of\n",
    "the data along that component.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663ec40-b3fe-4366-abbe-f304ffc354a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dac4f-3c88-4077-9360-5a5c8106969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8:\n",
    "   PCA uses the spread and variance of the data to identify principal components by finding the linear\n",
    "combinations of the original features that capture the most variance in the data. The first principal \n",
    "component is the linear combination that explains the most variance in the data, and each subsequent \n",
    "component captures the most variance that is orthogonal to the previous components. The principal components\n",
    "can then be used to create new features that capture the most important information in the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a4ba3-2ee2-4fc6-bd7d-ccc7023694f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e7803-944d-4cef-a7aa-5c8fc22bcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "9:\n",
    "  PCA handles data with high variance in some dimensions but low variance in others by identifying the\n",
    "principal components that capture the most variance in the data, regardless of whether that variance is \n",
    "high or low in any particular dimension. This means that PCA can effectively reduce the dimensionality of\n",
    "the data by focusing on the most important sources of variability, while ignoring the dimensions that have \n",
    "little impact on the overall structure of the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349c735-cb15-4f51-8add-b3bad3b12de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b1a22-8a5a-46fa-8ba5-5dbc5716b589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17547cfa-2d0c-4689-9c8b-2f129cd2fd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
